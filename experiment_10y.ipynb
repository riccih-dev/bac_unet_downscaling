{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment using dataset of 10 y\n",
    "run on google colab to utilize better gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boring Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!git clone https://riccih-dev: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@github.com/riccih-dev/bac_temp_downscaling.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# get changes\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from visualization.climate_data_visualizer import ClimateDataVisualizer\n",
    "from visualization.evaluation_visualizer import EvaluationVisualization\n",
    "from downscaling.pipeline import DownscalingPipeline\n",
    "from utility.utility import save_to_json, store_to_disk, load_via_url, split_data\n",
    "from model.modelconfig import UNetModelConfiguration\n",
    "from utility.data_generator import DataGenerator\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your desired directory\n",
    "new_directory = \"/content/bac_temp_downscaling/\"\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory)\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path='./results/'\n",
    "\n",
    "# Standardized Anomalies\n",
    "stats_file = './preprocessed_data/climatology_stats_sa_10y.json'\n",
    "file_preprocessed_era5 = './preprocessed_data/era5_preprocessed_standardized_anomalies_10y.nc'\n",
    "file_preprocessed_cerra = './preprocessed_data/cerra_preprocessed_standardized_anomalies_10y.nc'\n",
    "\n",
    "# Min Max\n",
    "#stats_file = './preprocessed_data/climatology_stats_MinMax_10y.json'\n",
    "#file_preprocessed_era5 = './preprocessed_data/era5_preprocessed_min_max_10y.nc'\n",
    "#file_preprocessed_cerra = './preprocessed_data/cerra_preprocessed_min_max_10y.nc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lr_data = xr.open_dataset(file_preprocessed_era5)\n",
    "preprocessed_hr_data = xr.open_dataset(file_preprocessed_cerra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_data, lr_val_data, lr_test_data, hr_train_data, hr_val_data, hr_test_data = split_data(preprocessed_lr_data, preprocessed_hr_data)\n",
    "\n",
    "train_data = [lr_train_data, hr_train_data]\n",
    "val_data = [lr_val_data, hr_val_data]\n",
    "test_data = [lr_test_data, hr_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functionality for Running the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_pipeline(normalization_type, train_data, val_data, model_setup, filename_suffix):\n",
    "    pipeline = DownscalingPipeline(normalization_type)\n",
    "\n",
    "    train_data_generator = DataGenerator(train_data[0], train_data[1], model_setup['batch_size'])\n",
    "    val_data_generator = DataGenerator(val_data[0], val_data[1], model_setup['batch_size'])\n",
    "\n",
    "    model = pipeline.fit_model(\n",
    "        train_generator = train_data_generator,\n",
    "        val_generator = val_data_generator,\n",
    "        scheduler_type = model_setup['scheduler_type'],\n",
    "        learning_rate_value = model_setup['learning_rate_value'],\n",
    "        num_epochs = model_setup['num_epochs'],\n",
    "        loss_type = model_setup['loss_type'],\n",
    "        filters = model_setup['filters']\n",
    "        #show_summary = True\n",
    "    )\n",
    "\n",
    "    pipeline.show_training_history(filename_suffix)\n",
    "\n",
    "    # Predict unseen data\n",
    "    result = pipeline.predict(lr_test_data, stats_file)\n",
    "\n",
    "    # evaluate predicted data\n",
    "    hr_test_denormalized = pipeline.denormalize(hr_test_data, stats_file)\n",
    "    metric_results = pipeline.evaluate_prediction(hr_test_denormalized, result)\n",
    "\n",
    "    visualizer = EvaluationVisualization()\n",
    "    visualizer.spatial_plots(hr_test_denormalized, result, filename_suffix)\n",
    "    visualizer.difference_maps(hr_test_denormalized, result, filename_suffix)\n",
    "    visualizer.histograms(hr_test_denormalized, result, filename_suffix)\n",
    "\n",
    "    history = pipeline.get_history()\n",
    "    save_to_json(filename_suffix, model_setup, history['loss'], history['val_loss'], metric_results, result_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_suffix = ''\n",
    "last_idx = 100\n",
    "\n",
    "normalization_types = ['standardized_anomalies', 'min_max']\n",
    "scheduler_types = ['step_decay', 'exponential_decay', 'time_decay'] \n",
    "learning_rate_values = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "loss_types = ['mse', 'mae', 'huber_loss'] \n",
    "num_epochs_list = [4, 10, 15, 20, 30, 50]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "initial_filters = [16, 32, 56, 64]\n",
    "\n",
    "model_configuration = UNetModelConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_type = normalization_types[0]\n",
    "scheduler_type = scheduler_types[0]\n",
    "learning_rate_value = learning_rate_values[2]\n",
    "num_epochs = num_epochs_list[0]\n",
    "batch_size = batch_sizes[2]\n",
    "loss_type = loss_types[2] \n",
    "initial_filter = initial_filters[1]\n",
    "filters = model_configuration.generate_filters(initial_filter)\n",
    "last_idx=0\n",
    "\n",
    "model_setup = {\n",
    "    'scheduler_type': scheduler_type,\n",
    "    'learning_rate_value': learning_rate_value,\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'loss_type': loss_type,\n",
    "    'filters': filters,\n",
    "    'activation_function': 'tanh', \n",
    "    'note': '10y, cropped area'\n",
    "}\n",
    "\n",
    "\n",
    "# ------------ step_decay ------------\n",
    "filename_suffix = '10y_sa_testing_'+str(last_idx)\n",
    "run_model_pipeline(normalization_type, train_data, val_data, model_setup, filename_suffix)\n",
    "last_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to drive\n",
    "- model_and_results_SUFFIX.json\n",
    "- histogram_plot_SUFFIX.png\n",
    "- difference_plot_SUFFIX.png\n",
    "- spatial_plot_SUFFIX.png\n",
    "- training_history_plot_SUFFIX.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /content/bac_temp_downscaling/results/ /content/drive/MyDrive/bac_results\n",
    "\n",
    "!cp -r /content/bac_temp_downscaling/results/ /content/drive/MyDrive/bac_results\n",
    "\n",
    "!cp -r /content/bac_temp_downscaling/results/ /content/drive/MyDrive/bac_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
