{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment using dataset of 10 y\n",
    "To maximize computational efficiency, the experiment was conducted on Google Colab, leveraging its superior GPU capabilities. The dataset spans a period of 10 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/drive/MyDrive/preprocessed_data/cerra_temporal_sa_10y.nc  /content/bac_temp_downscaling/data\n",
    "!cp -r /content/drive/MyDrive/preprocessed_data/era5_temporal_sa_10y.nc  /content/bac_temp_downscaling/data\n",
    "!cp -r /content/drive/MyDrive/preprocessed_data/climatology_stats_sa_10y.json /content/bac_temp_downscaling/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# get changes\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "from downscaling.pipeline import DownscalingPipeline\n",
    "from data_operations.utility import store_to_disk, split_dataset, store_to_disk\n",
    "from data_operations.data_loader import DataLoader\n",
    "from downscaling.modelconfig import UNetModelConfiguration\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory = \"/content/bac_temp_downscaling/\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "data_path = './data/climate_data/'\n",
    "result_path='./results/'\n",
    "data_split_path = './data/data_split/'\n",
    "preprocessed_path = './data/preprocessed_data/'\n",
    "era5_lsm_z_file = f'era5_lsm_z'\n",
    "cerra_lsm_orog_file = f'cerra_lsm_orog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZED ANOMALIES\n",
    "normalization_type = 'standardized_anomalies'\n",
    "file_cerra = 'cerra_sa_4y'\n",
    "file_era = 'era5_sa_4y'\n",
    "stats_file = preprocessed_path+'climatology_stats_sa_4y.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX\n",
    "normalization_type = 'min_max'\n",
    "stats_file =  preprocessed_path+'climatology_stats_mm_4y.json'\n",
    "file_cerra = 'cerra_mm_4y'\n",
    "file_era = 'era5_mm_4y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Already done locally, in 4y (with start_index 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "lr_train_data = DataLoader.load_from_disk('lr_train_sa_4y', data_split_path)\n",
    "hr_train_data = DataLoader.load_from_disk('hr_train_sa_4y', data_split_path)\n",
    "\n",
    "lr_val_data = DataLoader.load_from_disk('lr_val_sa_4y', data_split_path)\n",
    "hr_val_data = DataLoader.load_from_disk('hr_val_sa_4y', data_split_path)\n",
    "\n",
    "lr_test_data = DataLoader.load_from_disk('lr_test_sa_4y', data_split_path)\n",
    "hr_test_data = DataLoader.load_from_disk('hr_test_sa_4y', data_split_path)\n",
    "\n",
    "train_data = [lr_train_data, hr_train_data]\n",
    "val_data = [lr_val_data, hr_val_data]\n",
    "test_data = [lr_test_data, hr_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx=0\n",
    "\n",
    "normalization_types = ['standardized_anomalies', 'min_max']\n",
    "scheduler_types = ['step_decay', 'exponential_decay', 'time_decay'] \n",
    "learning_rate_values = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "loss_types = ['mse', 'mae', 'huber_loss'] \n",
    "num_epochs_list = [2, 4, 10, 15, 20, 30, 50]\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "initial_filters = [16, 32, 56, 64]\n",
    "\n",
    "model_configuration = UNetModelConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_suffix = 'sa_4y_'+str(last_idx)\n",
    "\n",
    "normalization_type = normalization_types[0]\n",
    "scheduler_type = scheduler_types[2]\n",
    "learning_rate_value = learning_rate_values[1]\n",
    "num_epochs = 1 #num_epochs_list[5]\n",
    "batch_size = 32#batch_sizes[0]\n",
    "loss_type = loss_types[2]\n",
    "initial_filter = initial_filters[2]\n",
    "filters = model_configuration.generate_filters(initial_filter)\n",
    "\n",
    "\n",
    "model_setup = {\n",
    "    'scheduler_type': scheduler_type,\n",
    "    'learning_rate_value': learning_rate_value,\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'loss_type': loss_type,\n",
    "    'filters': filters,\n",
    "    'activation_function': 'tanh',\n",
    "    'note': '4y, cropped area'\n",
    "}\n",
    "\n",
    "pipeline = DownscalingPipeline(normalization_type)\n",
    "pipeline.run_downscaling_pipeline(normalization_type=normalization_type, train_data=train_data, val_data=val_data, lr_test_data=lr_test_data, hr_test_data=hr_test_data, model_setup=model_setup, filename_suffix=filename_suffix, result_path=result_path, stats_file=stats_file)\n",
    "last_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /content/bac_temp_downscaling/results/ /content/drive/MyDrive/bac_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
