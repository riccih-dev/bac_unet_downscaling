{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing - Large Dataset (10y)\n",
    "contains steps for loading, preprocessing and storing climate data of the large data set (10y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from downscaling.pipeline import DownscalingPipeline\n",
    "from data_operations.data_loader import DataLoader\n",
    "from data_operations.utility import store_to_disk, split_dataset\n",
    "from data.urls import cerra_url, era5_url\n",
    "from IPython.display import display\n",
    "import os\n",
    "import xarray as xr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "data_path = './data/climate_data/'\n",
    "result_path='./results/'\n",
    "data_split_path = './data/data_split/'\n",
    "preprocessed_path = './data/preprocessed_data/'\n",
    "era5_lsm_z_file = f'era5_lsm_z'\n",
    "cerra_lsm_orog_file = f'cerra_lsm_orog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZED ANOMALIES\n",
    "normalization_type = 'standardized_anomalies'\n",
    "file_cerra = 'cerra_sa_4y'\n",
    "file_era = 'era5_sa_4y'\n",
    "stats_file = preprocessed_path+'climatology_stats_sa_4y.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX\n",
    "normalization_type = 'min_max'\n",
    "stats_file =  preprocessed_path+'climatology_stats_mm_4y.json'\n",
    "file_cerra = 'cerra_mm_4y'\n",
    "file_era = 'era5_mm_4y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Load via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DownscalingPipeline(normalization_type)\n",
    "cerra_t2m, cerra_lsm_orog, era5_t2m, era5_lsm_z = pipeline.load_climate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slow loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_laoder = DataLoader()\n",
    "era5_add_ds = data_laoder.load_via_url(era5_url.lsm_geop_url)\n",
    "store_to_disk(era5_lsm_z_file, era5_add_ds, data_path)\n",
    "\n",
    "cerra_add_ds = data_laoder.load_via_url(cerra_url.lsm_orog_url)\n",
    "store_to_disk(cerra_lsm_orog_file, cerra_add_ds, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_laoder = DataLoader()\n",
    "urls = era5_url.t2m_urls\n",
    "\n",
    "for url, idx_era5 in enumerate(urls):\n",
    "  era5_ds = data_laoder.load_via_url([url])\n",
    "  store_to_disk(\"era50\"+str(idx_era5), era5_ds, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_laoder = DataLoader()\n",
    "urls = cerra_url.t2m_urls\n",
    "\n",
    "for url, idx_cerra in enumerate(urls):\n",
    "  era5_ds = data_laoder.load_via_url([url])\n",
    "  store_to_disk(\"cerra0\"+str(idx_era5), era5_ds, data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat Climate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cerra = 131\n",
    "file_paths = [os.path.join(data_path, f'cerra0{i}.nc') for i in range(0, idx_cerra)]\n",
    "cerra_ds = xr.open_mfdataset(file_paths)\n",
    "\n",
    "size_in_bytes = cerra_ds.nbytes\n",
    "size_in_gb = size_in_bytes / (1024**3)\n",
    "\n",
    "print(f\"Size of the xarray dataset: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_era5 = 131\n",
    "file_paths = [os.path.join(data_path, f'era50{i}.nc') for i in range(0, idx_era5)]\n",
    "cerra_ds = xr.open_mfdataset(file_paths)\n",
    "\n",
    "size_in_bytes = cerra_ds.nbytes\n",
    "size_in_gb = size_in_bytes / (1024**3)\n",
    "\n",
    "print(f\"Size of the xarray dataset: {size_in_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_min = 8.3\n",
    "lat_min = 43.8\n",
    "lon_max = 14.5\n",
    "lat_max = 51.5\n",
    "crop_area = [lon_min, lat_min, lon_max, lat_max]\n",
    "\n",
    "pipeline = DownscalingPipeline(normalization_type)\n",
    "preprocessed_lr_data, preprocessed_hr_data = pipeline.preprocess_data(era5_ds, cerra_ds, era5_add_ds, cerra_add_ds, crop_region=crop_area, stats_filename=stats_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store preprocessed data\n",
    "To avoid repeating the preprocessing steps every time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./data/era5_preprocessed_standardized_anomalies_10y.nc\n",
      "Writing to ./data/cerra_preprocessed_standardized_anomalies_10y.nc\n"
     ]
    }
   ],
   "source": [
    "era5_preprocessed_file = f'era5_preprocessed_{normalization_type}_10y'\n",
    "cerra_preprocessed_file = f'cerra_preprocessed_{normalization_type}_10y'\n",
    "\n",
    "store_to_disk(era5_preprocessed_file, preprocessed_lr_data, preprocessed_path)\n",
    "store_to_disk(cerra_preprocessed_file, preprocessed_hr_data, preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ./data_split/lr_train_sa_same_stats_hr_10y.nc\n",
      "Writing to ./data_split/hr_train_sa_same_stats_hr_10y.nc\n",
      "Writing to ./data_split/lr_val_sa_same_stats_hr_10y.nc\n",
      "Writing to ./data_split/hr_val_sa_same_stats_hr_10y.nc\n",
      "Writing to ./data_split/lr_test_sa_same_stats_hr_10y.nc\n",
      "Writing to ./data_split/hr_test_sa_same_stats_hr_10y.nc\n"
     ]
    }
   ],
   "source": [
    "lr_train_data, lr_val_data, lr_test_data, hr_train_data, hr_val_data, hr_test_data = split_dataset(preprocessed_lr_data, preprocessed_hr_data)\n",
    "\n",
    "train_data = [lr_train_data, hr_train_data]\n",
    "val_data = [lr_val_data, hr_val_data]\n",
    "test_data = [lr_test_data, hr_test_data]\n",
    "\n",
    "store_to_disk('lr_train_sa_4y', lr_train_data, data_split_path)\n",
    "store_to_disk('hr_train_sa_4y', hr_train_data, data_split_path)\n",
    "\n",
    "store_to_disk('lr_val_sa_4y', lr_val_data, data_split_path)\n",
    "store_to_disk('hr_val_sa_4y', hr_val_data, data_split_path)\n",
    "\n",
    "store_to_disk('lr_test_sa_4y', lr_test_data, data_split_path)\n",
    "store_to_disk('hr_test_sa_4y', hr_test_data, data_split_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
