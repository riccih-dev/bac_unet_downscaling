{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscaling Experiment (4 years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "from downscaling.pipeline import DownscalingPipeline\n",
    "from data_operations.utility import store_to_disk, split_dataset, store_to_disk\n",
    "from data_operations.data_loader import DataLoader\n",
    "from downscaling.modelconfig import UNetModelConfiguration\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "data_path = './data/climate_data/'\n",
    "result_path='./results/'\n",
    "data_split_path = './data/data_split/'\n",
    "preprocessed_path = './data/preprocessed_data/'\n",
    "era5_lsm_z_file = f'era5_lsm_z'\n",
    "cerra_lsm_orog_file = f'cerra_lsm_orog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZED ANOMALIES\n",
    "normalization_type = 'standardized_anomalies'\n",
    "file_cerra = 'cerra_sa_4y'\n",
    "file_era = 'era5_sa_4y'\n",
    "stats_file = preprocessed_path+'climatology_stats_sa_4y.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX\n",
    "normalization_type = 'min_max'\n",
    "stats_file =  preprocessed_path+'climatology_stats_mm_4y.json'\n",
    "file_cerra = 'cerra_mm_4y'\n",
    "file_era = 'era5_mm_4y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xarray/backends/plugins.py:80: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Loading addtional features (lsm, z) from disk\n",
    "era5_add_ds = DataLoader.load_from_disk(era5_lsm_z_file, data_path)\n",
    "cerra_add_ds = DataLoader.load_from_disk(cerra_lsm_orog_file, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the xarray dataset: 17.10 GB\n",
      "Size of the xarray dataset: 2.65 GB\n"
     ]
    }
   ],
   "source": [
    "# Loading era5 and cerra data from disk\n",
    "idx_era5 =131\n",
    "start = 84 #4 y starting with index 84, index 0 would be start of 10 y\n",
    "idx_cerra = idx_era5\n",
    "\n",
    "# --- CERRA ---\n",
    "file_paths = [os.path.join(data_path, f'cerra0{i}.nc') for i in range(start, idx_cerra)]\n",
    "cerra_ds = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Get the size in bytes\n",
    "size_in_bytes = cerra_ds.nbytes\n",
    "size_in_gb = size_in_bytes / (1024**3)  \n",
    "print(f\"Size of the xarray dataset: {size_in_gb:.2f} GB\")\n",
    "\n",
    "# --- ERA5 ---\n",
    "file_paths = [os.path.join(data_path, f'era50{i}.nc') for i in range(start, idx_era5)]\n",
    "era5_ds = xr.open_mfdataset(file_paths)\n",
    "\n",
    "# Get the size in bytes\n",
    "size_in_bytes = era5_ds.nbytes\n",
    "size_in_gb = size_in_bytes / (1024**3)\n",
    "print(f\"Size of the xarray dataset: {size_in_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_min = 8.3\n",
    "lat_min = 43.8\n",
    "lon_max = 14.5\n",
    "lat_max = 51.5\n",
    "crop_area = [lon_min, lat_min, lon_max, lat_max]\n",
    "\n",
    "pipeline = DownscalingPipeline(normalization_type)\n",
    "preprocessed_lr_data, preprocessed_hr_data = pipeline.preprocess_data(era5_ds, cerra_ds, era5_add_ds, cerra_add_ds, crop_region=crop_area, stats_filename=stats_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preprocessed_lr_data, preprocessed_hr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store preprocessed data\n",
    "To avoid repeating the preprocessing steps every time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_preprocessed_file = f'era5_sa_4y'\n",
    "cerra_preprocessed_file = f'cerra_sa_4y'\n",
    "\n",
    "store_to_disk(era5_preprocessed_file, preprocessed_lr_data, preprocessed_path)\n",
    "store_to_disk(cerra_preprocessed_file, preprocessed_hr_data, preprocessed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lr_data = DataLoader.load_from_disk(file_era, preprocessed_path)\n",
    "preprocessed_hr_data = DataLoader.load_from_disk(file_cerra, preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(preprocessed_hr_data)\n",
    "display(preprocessed_lr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_data, lr_val_data, lr_test_data, hr_train_data, hr_val_data, hr_test_data = split_dataset(preprocessed_lr_data, preprocessed_hr_data)\n",
    "\n",
    "train_data = [lr_train_data, hr_train_data]\n",
    "val_data = [lr_val_data, hr_val_data]\n",
    "test_data = [lr_test_data, hr_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratios\n",
    "total_lr_data = len(preprocessed_lr_data.time)\n",
    "total_hr_data = len(preprocessed_lr_data.time)\n",
    "\n",
    "ratio_lr_train = len(lr_train_data.time) / total_lr_data\n",
    "ratio_lr_val = len(lr_val_data.time) / total_lr_data\n",
    "ratio_lr_test = len(lr_test_data.time) / total_lr_data\n",
    "\n",
    "ratio_hr_train = len(hr_train_data.time) / total_hr_data\n",
    "ratio_hr_val = len(hr_val_data.time) / total_hr_data\n",
    "ratio_hr_test = len(hr_test_data.time) / total_hr_data\n",
    "\n",
    "# Print ratios\n",
    "print(f\"Low-Resolution Data Ratios:\")\n",
    "print(f\"Train: {ratio_lr_train:.2%}, Validation: {ratio_lr_val:.2%}, Test: {ratio_lr_test:.2%}\")\n",
    "\n",
    "print(\"\\nHigh-Resolution Data Ratios:\")\n",
    "print(f\"Train: {ratio_hr_train:.2%}, Validation: {ratio_hr_val:.2%}, Test: {ratio_hr_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_to_disk('lr_train_sa_4y', lr_train_data, data_split_path)\n",
    "store_to_disk('hr_train_sa_4y', hr_train_data, data_split_path)\n",
    "\n",
    "store_to_disk('lr_val_sa_4y', lr_val_data, data_split_path)\n",
    "store_to_disk('hr_val_sa_4y', hr_val_data, data_split_path)\n",
    "\n",
    "store_to_disk('lr_test_sa_4y', lr_test_data, data_split_path)\n",
    "store_to_disk('hr_test_sa_4y', hr_test_data, data_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "lr_train_data = DataLoader.load_from_disk('lr_train_sa_4y', data_split_path)\n",
    "hr_train_data = DataLoader.load_from_disk('hr_train_sa_4y', data_split_path)\n",
    "\n",
    "lr_val_data = DataLoader.load_from_disk('lr_val_sa_4y', data_split_path)\n",
    "hr_val_data = DataLoader.load_from_disk('hr_val_sa_4y', data_split_path)\n",
    "\n",
    "lr_test_data = DataLoader.load_from_disk('lr_test_sa_4y', data_split_path)\n",
    "hr_test_data = DataLoader.load_from_disk('hr_test_sa_4y', data_split_path)\n",
    "\n",
    "train_data = [lr_train_data, hr_train_data]\n",
    "val_data = [lr_val_data, hr_val_data]\n",
    "test_data = [lr_test_data, hr_test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx=0\n",
    "\n",
    "normalization_types = ['standardized_anomalies', 'min_max']\n",
    "scheduler_types = ['step_decay', 'exponential_decay', 'time_decay'] \n",
    "learning_rate_values = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "loss_types = ['mse', 'mae', 'huber_loss'] \n",
    "num_epochs_list = [2, 4, 10, 15, 20, 30, 50]\n",
    "batch_sizes = [2, 4, 8, 16, 32, 64]\n",
    "initial_filters = [16, 32, 56, 64]\n",
    "\n",
    "model_configuration = UNetModelConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization for Standardized Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_suffix = 'sa_4y_'+str(last_idx)\n",
    "\n",
    "normalization_type = normalization_types[0]\n",
    "scheduler_type = scheduler_types[2]\n",
    "learning_rate_value = learning_rate_values[1]\n",
    "num_epochs = num_epochs_list[5]\n",
    "batch_size = batch_sizes[1]\n",
    "loss_type = loss_types[2]\n",
    "initial_filter = initial_filters[2]\n",
    "filters = model_configuration.generate_filters(initial_filter)\n",
    "\n",
    "\n",
    "model_setup = {\n",
    "    'scheduler_type': scheduler_type,\n",
    "    'learning_rate_value': learning_rate_value,\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'loss_type': loss_type,\n",
    "    'filters': filters,\n",
    "    'activation_function': 'tanh',\n",
    "    'note': '4y, cropped area'\n",
    "}\n",
    "\n",
    "pipeline = DownscalingPipeline(normalization_type)\n",
    "pipeline.run_downscaling_pipeline(normalization_type=normalization_type, train_data=train_data, val_data=val_data, lr_test_data=lr_test_data, hr_test_data=hr_test_data, model_setup=model_setup, filename_suffix=filename_suffix, result_path=result_path, stats_file=stats_file)\n",
    "last_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
