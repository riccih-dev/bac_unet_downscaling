{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility.Loader.DataLoaderService import load_data\n",
    "from model.UNet import UNetModel\n",
    "from DownscalingPipeline import DownscalingPipeline\n",
    "from IPython.display import display\n",
    "import xarray as xr\n",
    "import dask\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "## ERA5 and CERRA Data\n",
    "I've loaded the data via cloud from https://storage.ecmwf.europeanweather.cloud/Code4Earth/. In total I used the data for the time period from 2019-01 - 2021-06 for both data sets (ERA5 and CERRA). While fetching the data from the cloud I've realized that ERA5 and CERRA data only exists for the time period till 2021-06.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Which Parameters do I need?\n",
    "From both data set types (ERA5 and CERRA):\n",
    "- 2m temperature\n",
    "- time (year, month, day, time)\n",
    "- longitude\n",
    "- latitude\n",
    "\n",
    "In addition to the ERA5 and CERRA datasets containing the t2m-parameter, there are two datasets containting some additional parameters. The parameters are the following:\n",
    "<br>\n",
    "\n",
    "**ERA5**\n",
    "<br>era5_topography_ds contains lsm and z as data variables\n",
    "- **lsm**  => Land Surface Model. It refers to land surface variables that describe various properties and processes related to the land surface\n",
    "- **z** => refers to geopotential height, it provides information about the vertical structure of the atmosphere and is often used to analyze weather patterns, identify atmospheric circulation features, and understand large-scale atmospheric dynamics\n",
    "<br>\n",
    "\n",
    "**CERRA**\n",
    "<br>cerra_orography_ds contains lsm and orog as data variables\n",
    "- **lsm** => refers to land surface variables that describe various properties and processes related to the land surface\n",
    "- **orog** => topography or relief of the Earth's surface\n",
    "\n",
    "These datasets are only available for one time period. But since the orography etc. doesn't change during such a short time period, I fetched it once from the cloud and the added it as a new dimension to the exisitng ERA5 and CERRA datasets.\n",
    "\n",
    "<br>\n",
    "\n",
    "### How did I split the data?\n",
    "As discussed with Irene I'm using for the time period, only few years and the following splitting (ca. 70 - 15 - 15):\n",
    "\n",
    "- **trainig:** 2015 - 2019 \n",
    "- **testing:** 2020 \n",
    "- **validation:** 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xarray/backends/plugins.py:80: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "era5_list, cerra_list = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "## Data Preperation\n",
    "Currently loaded \n",
    "- CERRA 01-2019 - 2021-06\n",
    "- ERA5 01-2019 - 2021-06\n",
    "\n",
    "### 01 Combine \n",
    "Combine list of cerra xarrays to single xarray (same for era5) and save it to disk.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_disk(file_name, data, file_path=\"./data/\"):\n",
    "    \"\"\"\n",
    "    Store data to disk in a specified folder.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name (str): The name of the file to be saved.\n",
    "    - data: The data to be stored.\n",
    "    - folder_path (str, optional): The path to the folder where the file will be saved.\n",
    "      Default is \"data\".\n",
    "    \"\"\"\n",
    "    file = f\"{file_path}{file_name}.nc\"\n",
    "\n",
    "    #data.load().to_netcdf(file)\n",
    "    #data.to_netcdf(f\"{file_path}{file_name}.nc\")\n",
    "\n",
    "    # Assuming 'ds' is your xarray dataset\n",
    "    #data.to_zarr(f\"{file_path}{file_name}.nc\", mode='w', compute=True, scheduler='threads', encoding={'variable_name': {'compressor': {'id': 'zstd', 'level': 3}}})\n",
    "\n",
    "    data.to_zarr(file, mode='w', compute=False)\n",
    "\n",
    "    # Use dask to parallelize the computation and writing\n",
    "    dask.compute(data.compute())\n",
    "\n",
    "\n",
    "\n",
    "def load_from_disk(file_name, file_path=\"./data/\"):\n",
    "    \"\"\"\n",
    "    Load climate data from disk.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the directory containing the file. Default is \"data\".\n",
    "    - file_name (str): Name of the file.\n",
    "\n",
    "    Returns:\n",
    "    - xr.Dataset: Loaded climate data.\n",
    "    \"\"\"\n",
    "    file_path = file_path + file_name + \".nc\"\n",
    "    return xr.open_dataset(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to store the single xarray of cerra & era after cropping to the disk. So I will save some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine \n",
    "cerra_ds = xr.concat(cerra_list, dim='time')\n",
    "era5_ds = xr.concat(era5_list, dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Crop the Data set\n",
    "**Problem** The input shape is not dividable by 32 => so i have crop it, so the model works proberly. \n",
    "\n",
    "#### Explaination\n",
    "In many convolutional neural network architectures, especially those that involve multiple downsampling and upsampling operations, it is common to design the network such that the spatial dimensions are divisible by certain factors (e.g., 32). This ensures that the dimensions can be downsampled and upsampled without resulting in fractional spatial dimensions.\n",
    "\n",
    "For example, if you're using strides of 2 in downsampling operations and upsampling by factors of 2, the spatial dimensions of your feature maps need to be divisible by 2.<br>\n",
    "\n",
    "#### TODO: \n",
    "later cut it a little bit better (from each edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen({'longitude': 281, 'latitude': 221, 'time': 7296})\n",
      "Frozen({'longitude': 256, 'latitude': 192, 'time': 7296})\n",
      "Frozen({'longitude': 801, 'latitude': 501, 'time': 7296})\n",
      "Frozen({'longitude': 800, 'latitude': 480, 'time': 7296})\n"
     ]
    }
   ],
   "source": [
    "# ERA 5 \n",
    "# Desired divisible factor (e.g., 32)\n",
    "divisible_factor = 32\n",
    "\n",
    "# Calculate the new size that is divisible by the factor for both longitude and latitude\n",
    "new_longitude_size = (era5_ds.longitude.size // divisible_factor) * divisible_factor\n",
    "new_latitude_size = (era5_ds.latitude.size // divisible_factor) * divisible_factor\n",
    "\n",
    "# Crop the ERA5 dataset\n",
    "era5 = era5_ds.sel(\n",
    "    longitude=slice(era5_ds.longitude.values[0], era5_ds.longitude.values[new_longitude_size - 1]),\n",
    "    latitude=slice(era5_ds.latitude.values[0], era5_ds.latitude.values[new_latitude_size - 1])\n",
    ")\n",
    "\n",
    "# Verify the new dimensions\n",
    "print(era5_ds.dims)\n",
    "print(era5.dims)\n",
    "\n",
    "\n",
    "# CERRA \n",
    "# Calculate the new size that is divisible by the factor for both longitude and latitude\n",
    "new_longitude_size = (cerra_ds.longitude.size // divisible_factor) * divisible_factor\n",
    "new_latitude_size = (cerra_ds.latitude.size // divisible_factor) * divisible_factor\n",
    "\n",
    "# Crop the ERA5 dataset\n",
    "cerra = cerra_ds.sel(\n",
    "    longitude=slice(cerra_ds.longitude.values[0], cerra_ds.longitude.values[new_longitude_size - 1]),\n",
    "    latitude=slice(cerra_ds.latitude.values[0], cerra_ds.latitude.values[new_latitude_size - 1])\n",
    ")\n",
    "\n",
    "# Verify the new dimensions\n",
    "print(cerra_ds.dims)\n",
    "print(cerra.dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cropped data to disk\n",
    "#store_to_disk(\"cerra_01_2019_06_2021\", cerra)\n",
    "#store_to_disk(\"era5_01_2019_06_2021\", era5)\n",
    "\n",
    "#jobs = [(\"cerra_01_2019_06_2021\", cerra), (\"era5_01_2019_06_2021\", era5)]\n",
    "#Parallel(n_jobs=-1)(delayed(store_to_disk)(job[0], job[1]) for job in jobs)\n",
    "\n",
    "store_to_disk(\"era5_01_2019_06_2021\", era5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zarr is better suited to parallel writes. Even with arrays backed with distributed task clusters, to_netcdf brings the array to the local thread (in chunks, but still) to write to the netcdf in the main thread. Writing with zarr schedules the write, then the workers write to the storage in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to era5_01_2019_06_2021\n"
     ]
    }
   ],
   "source": [
    "file = \"era5_01_2019_06_2021\"\n",
    "write_job = era5.to_netcdf(file, compute=False) #try zarr\n",
    "\n",
    "print(f\"Writing to {file}\")\n",
    "write_job.compute()\n",
    "\n",
    "# time: 10.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunantly the data is completely gone D: \n",
    "When you use mode='w' with to_zarr, it means you are writing a new dataset, and if files with the same name already exist, they will be replaced.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "#write_job = cerra.to_netcdf(file, compute=False) \n",
    "# try zipping (to save storage on disk )\n",
    "\n",
    "write_job = cerra.to_zarr(path, mode='w', compute=False)\n",
    "\n",
    "print(f\"Writing to {file}\")\n",
    "write_job.compute(progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 03 Align Spatial Dimension \n",
    "So both cerra & era5 cover almost same spatial space<br>\n",
    "initially, no alignment needed => ERA5 covers a little bit more spatial space (but thats okay, as in covers orginally the whole globe, at a padding is desired so the boardes of cerra can be correctly predicted )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW MAX & MIN OF COORDINATES\n",
    "print(\"ERA\")\n",
    "print(\"latitude: \")\n",
    "print(cropped_era5_data.latitude.max().values)\n",
    "print(cropped_era5_data.latitude.min().values)\n",
    "print(\"longitude: \")\n",
    "print(cropped_era5_data.longitude.max().values)\n",
    "print(cropped_era5_data.longitude.min().values)\n",
    "print(\"latitude: \")\n",
    "print(era5.latitude.max().values)\n",
    "print(era5.latitude.min().values)\n",
    "print(\"longitude: \")\n",
    "print(era5.longitude.max().values)\n",
    "print(era5.longitude.min().values)\n",
    "\n",
    "print(\"\\nCERRA\")\n",
    "print(\"latitude: \")\n",
    "print(cerra.latitude.max().values)\n",
    "print(cerra.latitude.min().values)\n",
    "print(\"longitude: \")\n",
    "print(cerra.longitude.max().values)\n",
    "print(cerra.longitude.min().values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 04 Split Data Set \n",
    "Split both sets into training, testing, validation \n",
    "\n",
    "**Disclaimer:** Splitting of cerra data takes very long. I try to speed it up, by paralizing the job\n",
    "\n",
    "##### Parallelizing the job\n",
    "To speed up the data splitting process, you can consider parallelizing the splitting operations using the joblib library. The joblib library allows you to parallelize the computation, which can be particularly beneficial when dealing with large datasets. <br><br> The split_data function is defined to split the time dimension for a given dataset (ERA5 or CERRA). The Parallel function from joblib is then used to parallelize the splitting process for both ERA5 and CERRA datasets. <br><br> \n",
    "In the context of the joblib library, the parameter -1 for the n_jobs argument means to use all available CPU cores. It is a shorthand for specifying the number of parallel jobs to run in parallel processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk (no need to combine & crop it everytime)\n",
    "cerra = load_from_disk(\"cerra_01_2019_06_2021\")\n",
    "era5 = load_from_disk(\"era5_01_2019_06_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_(t, data):\n",
    "    return data.sel(time=t)\n",
    "\n",
    "def split(args):\n",
    "    t, data = args\n",
    "    return data.sel(time=t)\n",
    "\n",
    "def split_data(common_time, data):\n",
    "    train_time, test_time = train_test_split(common_time, test_size=0.2, shuffle=False)\n",
    "    train_time, val_time = train_test_split(train_time, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Use joblib to parallelize the following operations\n",
    "    train_data, val_data, test_data = Parallel(n_jobs=-1)(delayed(split)((t, data)) for t in [train_time, val_time, test_time]) # type: ignore\n",
    "    return train_data, val_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (longitude: 256, latitude: 192, time: 4668)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float32 -25.0 -24.75 -24.5 ... 38.25 38.5 38.75\n",
      "  * latitude   (latitude) float32 75.0 74.75 74.5 74.25 ... 27.75 27.5 27.25\n",
      "  * time       (time) datetime64[ns] 2019-01-01 ... 2020-08-06T09:00:00\n",
      "Data variables:\n",
      "    t2m        (time, latitude, longitude) float32 248.5 248.6 ... 304.7 304.8\n",
      "    z          (time, latitude, longitude) float32 1.797e+04 ... 9.578e+03\n",
      "    lsm        (time, latitude, longitude) float32 1.0 1.0 1.0 ... 1.0 1.0 1.0\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    history:      2023-05-26 07:39:28 GMT by grib_to_netcdf-2.28.0: grib_to_n... <xarray.Dataset>\n",
      "Dimensions:    (longitude: 256, latitude: 192, time: 1168)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float32 -25.0 -24.75 -24.5 ... 38.25 38.5 38.75\n",
      "  * latitude   (latitude) float32 75.0 74.75 74.5 74.25 ... 27.75 27.5 27.25\n",
      "  * time       (time) datetime64[ns] 2020-08-06T12:00:00 ... 2020-12-30T09:00:00\n",
      "Data variables:\n",
      "    t2m        (time, latitude, longitude) float32 274.7 274.8 ... 290.2 291.4\n",
      "    z          (time, latitude, longitude) float32 1.797e+04 ... 9.578e+03\n",
      "    lsm        (time, latitude, longitude) float32 1.0 1.0 1.0 ... 1.0 1.0 1.0\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    history:      2023-05-26 07:39:28 GMT by grib_to_netcdf-2.28.0: grib_to_n... <xarray.Dataset>\n",
      "Dimensions:    (longitude: 256, latitude: 192, time: 1460)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float32 -25.0 -24.75 -24.5 ... 38.25 38.5 38.75\n",
      "  * latitude   (latitude) float32 75.0 74.75 74.5 74.25 ... 27.75 27.5 27.25\n",
      "  * time       (time) datetime64[ns] 2020-12-30T12:00:00 ... 2021-06-30T21:00:00\n",
      "Data variables:\n",
      "    t2m        (time, latitude, longitude) float32 243.7 243.7 ... 305.8 305.6\n",
      "    z          (time, latitude, longitude) float32 1.797e+04 ... 9.578e+03\n",
      "    lsm        (time, latitude, longitude) float32 1.0 1.0 1.0 ... 1.0 1.0 1.0\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    history:      2023-05-26 07:39:28 GMT by grib_to_netcdf-2.28.0: grib_to_n...\n"
     ]
    }
   ],
   "source": [
    "# Parallelize the splitting for both ERA5 and CERRA\n",
    "common_time = era5['time'].values\n",
    "\n",
    "era5_train, era5_val, era5_test = split_data(common_time, era5)\n",
    "#print(era5_train, era5_val, era5_test)\n",
    "\n",
    "store_to_disk(\"era5_train_2019_2021\", era5_train)\n",
    "store_to_disk(\"era5_val_2019_2021\", era5_val)\n",
    "store_to_disk(\"era5_test_2019_2021\", era5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cerra_train, cerra_val, cerra_test = split_data(common_time, cerra)\n",
    "print(cerra_train, cerra_val, cerra_test)\n",
    "\n",
    "store_to_disk(\"cerra_train_2019_2021\", cerra_train)\n",
    "store_to_disk(\"cerra_val_2019_2021\", cerra_val)\n",
    "store_to_disk(\"cerra_test_2019_2021\", cerra_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### 01 - Find Input Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extracting dimensions from the dataset\n",
    "time_steps = len(era5['time'])\n",
    "latitude_points = len(era5['latitude'])\n",
    "longitude_points = len(era5['longitude'])\n",
    "num_variables = len(era5.data_vars)  # Assuming all data variables are used as input\n",
    "\n",
    "# Reshaping into the input shape\n",
    "input_shape = (time_steps, latitude_points, longitude_points, num_variables)\n",
    "input_shape = (latitude_points, longitude_points, num_variables)  # like irenes model (she doesn't use time)\n",
    "\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_service = UNetModel()\n",
    "model = model_service.create_model(input_shape)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model \n",
    "For Later: look into TimeSeriesSplit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why I stored my data to disk:\n",
    "Storing your preprocessed and split dataset to disk is a common and efficient practice, especially when working with large datasets. This can save you time during debugging and testing phases, as you won't need to repeatedly preprocess and split the data, which can be time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# CERRA\n",
    "cerra_train = load_from_disk(\"cerra_train_2019_2021\")\n",
    "cerra_val = load_from_disk(\"cerra_val_2019_2021\")\n",
    "cerra_test = load_from_disk(\"cerra_test_2019_2021\")\n",
    "\n",
    "# ERA5\n",
    "era5_train = load_from_disk(\"era5_train_2019_2021\")\n",
    "era5_val = load_from_disk(\"era5_val_2019_2021\")\n",
    "era5_test = load_from_disk(\"era5_test_2019_2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='logs')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=era5_train,  # Replace with your training data\n",
    "    y=cerra_train,  # Replace with your training labels\n",
    "    validation_data=(validation_data, validation_labels),  # Replace with your validation data\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
