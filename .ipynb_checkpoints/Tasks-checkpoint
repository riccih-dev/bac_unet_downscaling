# TODOs:
o **implementation:**
    o adjust predict (so it works correctly wiht pre/post processing)

o **Improve model:**
    o SA vs. min/max pre processing
        o try both approaches - which peforms better?
        o create graph & include in thesis 
    o try different geopgraphical areas for era5
        o era5 should be 10 % larger, than europe to get border effects
    o normalize lsm and z/orog as well ?
        o additional Features (lsm, z/orog): The decision to normalize these features depends on their scales and distributions. If these variables have different scales or widely varying magnitudes, normalization can help the model learn more efficiently.
        o irene normalized this data
        o APPROACH: normalize them + later check which works better
        o put result into thesis 
    o where to use parallelism as well - to speed up runtime
    o crop or padding for LR data to match dimension of hr data
        o initial start with padding
        o maybe later try also crop & see which works better
    o abstract data_service into multiple classes ? 

o **might not be important**
    o find a way to save large data sets (before / after training split)

o **Thesis**
    o read more papers 
        o Machine Learning-Based Estimation ...
        o Climate Model-driven ... 
        o A deep learning model for forecasting global monthly mean sea surface temperature anomalies
        o WF-UNet: Weather Data Fusion using 3D-UNet for Precipitation
        o Ensemble PostProcessing of Daily Percipation ... (Quick Ready Through)
        o https://github.com/ECMWFCode4Earth/DeepR/tree/main
    o Add missing Picures:
        o UNet Architecture
        o UNet Model
        o Table with Variables
        o Picture of European Region 
        o Picture of Downscaling
        o Picture LR vs HR European Region or smaller region - to see HR is more detailed 
    o check if i added everywhere references
    o check if text is still valid 
    o Add new infos to text
        o Infos about Test Data Splitting
        o Infos about Data (years, features etc.)
        o Add Info, that Model is based on Model of Irene & Sha
        o add where i get the base model idea from (refer to code as in https://github.com/ECMWFCode4Earth/tesserugged/blob/master/dev/unet/UNET_documentation.ipynb)
        o pipeline
        o pre & post processing 
    o Write about missing parts 
        o Evaluation
        o Results
        o Conclusio & Further Work
        o Acknowledgment 
        o Finidngs in Parameter Selection
        o Add Values in UNet Architecture 
    o add info, that era5 covers 10 % geopgraphical land



## LAST TASKS
o Go through code
    o add docs
    o remove unecessary comments
    o simplify complex code 

o thesis:
    o proof reading
    o all pictures & tables are there?
    o plagiatscheck 



# Working on Uni Server
o Uni Server - Mail von Ewald Hopp lesen
    o https://webmail2016.univie.ac.at/?_task=mail&_action=show&_uid=2893&_mbox=INBOX&_extwin=1
    o https://wiki.univie.ac.at/display/DataMining/Data+Mining+and+Machine+Learning


# Daily Log
## past
### 29-11
x autoloading doesnt work => solution %autoreload now

x fix map for cerra
x load cerra data to get structure (chat)
x ds[variables[1]] => test this for cerra (i think there's the error)
=> i forogt to use the correct variable (,:

x find coordinate range from cerra 
x apply coordinate range to era5 (won't use )
x check new map

x load_data_from_disk doesnt work (only for CERRA; for ERA5 it does work)
=> used wrong path: fixed:D

### 7-12
x Unterstanding Programm von Irene
x make Notes about Irene's Program

### 8-12
x Unterstanding program von Irene
x make Notes about Irene's Program

x unterstand program from Y sha
x make notes about model from Y sha

x adjust my unet model accordingly

x Make Notes & Overview about UNet Model (for moments when i don't unterstand anymore what I'm doing :D )

x Unterstand my Unet Model

x look into running computations on work vserver

x get infos about
    x decoder incl. maxpooling
    x encoder incl. conv2dtranspose


### 10-12
o get infos about
    x Backpropagation 
    x learning schedular from keras as callback for model fit vs. learning rate
    x batch size
    x epochs 
    x unterstand resolution between cerra & era5 (keep all data from cerra?)

x find better var names
x write down what everything does as comment

x check which approach is better to incoporate addtional features



### 17-12
x find input shape
x revise unet model & notes
x CERRA & ERA5 not a list of multiple xarrays but one xarray containing all 


### 20-12
o fix shape for decoder 
o adjust code to cut era5 so it is dividable by 32

model seems to work so far, now i only have to evaluate it 
there


### 27-12 
x revise UNet Model approach
x clean up the code

### 28-12
x ask irene if she has time for a meeting
x write mail to irene 
x write mail to lukas
x write docs for ipynb 
x clean up ipynb 


### 30-12
x answer to irenes mail 
x clean up code (go through classes)

**Note**
i made an mistake and deleted unfortunantly all of the old data i got D: now i have to wait for irene. 


### 01-01    
x make an process overview about pipeline & experiement functionalities

### 03-01
x Answer Mail to Irene 
    x find out how many years i do need 
x infos wegen Uni Server gesammelt

x put things from experiment into pipeline
x build new experiment file


**Weitere Infos**
- Bekomme wieder einen Zugriff auf Uni Server
- Irene stellt mir die Daten zu Verfügung
- Ich werde mir von der Arbeit einen Webhosting nehmen und dort meine Daten zu Verfügung stellen 


### 04-01 
x Webserver set up 
x load era5 data to server
x downloaed era5 from server into xarray 

x adjust DataLoader File
    x new config file with all urls in one array
    x remove generate_filenmes
    x only use xarray loader
x adjust era5_loader 
x adjust cerra_loader

x add references in thesis (introduction)

### 10-01
x set up ftp server account for irene 
x answer irene with ftp server 

x add references (related work)
x add references (problem setting) - no at all ? 
x add references methodologies  


### 12-01
o add ref for Dabernig et al. (2017b) and Stauffer et al. (2017)
o which references are still missing

o get infos about standardization & it's effect
o differences between SAMOS und normal preprocessing & it's effect
o implement both approaches 

o make a back up copy of data

x go through tasks and see what's still relevant 


### 18-01
x Daten Laden
    x Code aufräumen
    x ERA5 File URL in config anlegen 
    x CERRA FIle URLS in config anlegen
    x look at lsm_orog_url data for cerra 
    x look at lsm_orog_url data for era5 (how to load it, into one var?)
    x load 2 years of cerra
    x load 2 years of era5
    x find out why 
x Mail an Irene 

x Preprocessing verstehen
    x was wird allgemein damit gemacht?
    x base preprocessing finden => Z-normalization
    x was will ich damit machen - unet model? 
        x irenes erstes model anschauen


### 19-01 Fr
15:30 - 17:30
x Standardized Anomlaies
    x Text vom Kollegen verstehen (schlimmstenfall absatzweise in chatgpt)
    x Überlegen wie anwenden für UNet (nur postprocessing oder auch pre-processing, immer pre & post processing?)
x Unterschied zwischen SA und z-norm? => seems to be the same 
    x falls es keinen Unterschied gibt, dann Min/Max Processing nehmen (wie in Tessegrid)
x Was ist SAMOS
    x more info in notion basics festhalten

x main idea of SA - paper geben und schauen wie für meins ableiten kann, mit refernencen 
x reference finden, warum min/max processing nehmen als baseline  

x Implementierung Pre-Processing
    x alle Steps in der pipeline vorhanden
    x crop data to be dividable 
        x better method name
        x cropping from each side
        x test if cropping works correclty
    x Standardized Anomalies
    x Min/Max


MISSING 
o Metrik implementieren 
    o Welche Varianten verwenden Irene & Co (zotero papers)
    o Implementieren dieser Varianten
    o in notion vermerken
        o was messen sie
        o was würde resultat aussagen 

o Prediction
    o works correctly with pre/post processing
    o add post processing to min/max (if needed?)

### 20-01 Saturday
x Debuggen
    x fix MinMax PreProcesser
    x find out why lsm for lr only nan even if should be 0-1 values 
    x initial setting of param -> later optimizing hyperparameter
    o fit fit_model
        x unterstand dimension mismatch error
        x create function for padding
    x test loading
    x test preprocess_data
    x test split_data

### 21-01 Sunday
x Debuggen
    x padding
        x clean code
        x test for addtional features
        x add padding to preprocessing
        x pad 10 % plus of cerra
        x padding also for the addtional era5 data?
        x clean up code for pad 10 % plus of cerra
    x crop era5
        x only + 5 % of cerra 
        x before or after cropping? still needs to be divisable by 32 ! 
    o preprocess
        x crop cerra to divisable by 32
        x crop era5 to cerra area + 5 %
        x pad era5 to match hr dimension
        x normalize (hopefully it does work normally with nan)
        x pad cerra plus 5 % (to matche era5 outer kayer)
    x test pre processing again



## currently
### 22-02 Monday 
GOAL für heute: Model einmal laufen lassen + debugged 

o Debuggen
    o clean experiement        
    x test & fix peprocessing
        x try another factor (2,4,8)
        x decide order of individual steps
        x fix pad_lr_data_to_match_hr (only nan values?)
        x clean code 
        x test both normalization strategies
    o test & adjust fit_model
        o fix order of dimensions 
        o add lsm & z to fit

o Model lokal laufen lassen mit super kleinen DS 
    o Training: 1 Monat, Validation: 1 Monat, Test: 1 Monat (differnt years - same Month)
    o Training: 4 Monate (05, 06 from 2006, 2007), Validation: 05,06-2008, Test: 05,06-2009

o Während das Model läuft -> Paper lesen:
    o Reference finden - Standardized Anomalies
    o Phrases finden 
    o Ideen für Struktur finden 


 

### ---- DUMP noch zum erledigen --- 

o Weiter Debuggen:
    o Fix Predict Method 
        o create method in min / max preprocessor
        o create t2m normalizer for only one var for existing normalizer stats
    o normalizer: try approach as irene with dim & check difference
    fix/adjust fit_model 
        o add optimizer
        o add early stopping 
    o test fit_model
    o test predict
    o test metrics
    o text visualization
    o always sorted? same order of dimensions?

Read & Mark:
    o Machine Learning-Based Estimation ...
    o Climate Model-driven ... 
    o A deep learning model for forecasting global monthly mean sea surface temperature anomalies
    o WF-UNet: Weather Data Fusion using 3D-UNet for Precipitation
    o Ensemble PostProcessing of Daily Percipation ... (Quick Ready Through)

o **Preprocessing** 
    o in Thesis beschreiben (Infos aus Notion nehmen: Texts, Ideas & Refs to add to Thesis)
    o was damit bezwecken (Infos aus Notion nehmen)
    o die zwei varianten beschreiben
        o min/max (reference zu "A deep learning model for forecasting global monthly mean sea surface temperature anomalies" - in zotero gespeichert)
        o SA
    o sagen warum beide varianten implementiert (für den Vergleich, base & meine effizientere Art)

o Write about the **Pipeline / Process**
    o (Infos aus Notion nehmen: Texts, Ideas & Refs to add to Thesis)

o **Metrik** in Thesis beschreiben
    o Welche Arten
    o warum, falls es einen Grund gibt

**Server Verbindung**
o Verbindung zum Server testen
o Test Skript laufen lassen
    o Output speichern
    o wie lässt man skript laufen
    o wie stoppt man skript
o Mail an Lukas wie das mit Server reservieren funktioniert (?)

o **Model am Server laufen lassen**
    o Training: 1 Monat, Validation: 1 Monat, Test: 1 Monat (differnt years - same Month)
    o Training: 4 Monate (05, 06 from 2006, 2007), Validation: 05,06-2008, Test: 05,06-2009
    o Training: ca 4 Jahre, Validation: ca 1 Jahr, Test: ca 1 Jahr 

o **Optimieren Planen**
    o Werte fürs Optimieren raussuchen
    o Verstehen, wie diese Werte funktionieren
    o Anfangswerte + weitere Varianten festlegen 

**Testen**
o Welche Preprocessing Variante besser
    o Ergebnis visualisieren
    o Ergebnis in Thesis festhalten
    o Erklären warum besser

o Hyperparameter Optimierung 
    o Optimierungsresultate festhalten 
    o Während das Model läuft, weitere Optimierungsmöglichkeiten recherchieren 
    o in Thesis festhalten
        o Welche Hyperparametere ich optimiert habe
        o Was diese Hyperparameter machen
        o Wie ich bei der Optimierung vorgegangen bin / Werte etc. 
        o Falls ich schon Resultate habe, diese beschreiben

o Mail an Irene wegen Präsentation schreiben 

o **Conclusio** schreiben
    o Was habe ich gemacht
    o Ergebnis zusammenfassen
    o Informieren über weitere Vorgehensweisen zu Optimierung 
    o Weitere Vorgehensweise zur Optimierung beschreiben

o **Acknowledgment** beschreiben
    o 

o


## next
### 23-02 Tuesday
14:00 - 17:30 TU Bib(3.5 h)



### 24-02 Wednesday
14:00 - 19:00 TU Bib (4h)

19:30 - 21:00 Leesesaal draußen (1.5h)


### 25-02 Thursday
14:00 - 16:00 Home (2h)


### 26-02 Saturday
6:30 - 9:30 Home (3h)


14:00 - 17:00 AK (3h)


### 27-02 Sunday
9:00 - 12:00 Home (3h)

16:00 - 20:00 AKH Bib (4h)


# Some Site Notes
### AUTOLOADING
**%autoreload** now (also %autoreload) - perform autoreload immediately.<br>
**%autoreload off** (also %autoreload 0) - turn off autoreload.<br>
**%autoreload explicit** (also %autoreload 1) - turn on autoreload only for modules whitelisted by %aimport statements.<br>
**%autoreload all** (also %autoreload 2) - turn on autoreload for all modules except those blacklisted by %aimport statements. <br>
**%autoreload complete** (also %autoreload 3) - all the fatures of all but also adding new objects from the imported modules (see IPython/extensions/tests/test_autoreload.py::test_autoload_newly_added_objects).

=> mit %autoreload now funktioniert es endlich :D





# Mit Irene besprechen
In meiner Vorgehensweise, habe ich SA (nicht wie in Paper ihres Kollegen) nur im post-processing step genutzt, sondern für Pre- & Post-procesing, d.h. das model damit trainiert.
Passt das so? Oder etwas falsch verstanden?